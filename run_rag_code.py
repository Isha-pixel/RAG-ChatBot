# -*- coding: utf-8 -*-
"""Run_RAG_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jB-qFGRZLTR8IpzgIJC8Xj2U8YCFr9kb
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/AlaGrine/RAG_chatabot_with_Langchain.git
# %cd RAG_chatabot_with_Langchain

!pip install langchain==0.1.4
!pip install langchain-google-genai==0.0.6
!pip install langchain-openai==0.0.2.post1
!pip install docx2txt
!pip install chromadb
!pip install huggingface_hub
!pip install cohere
!pip install streamlit==1.28.0
!pip install --upgrade tiktoken
!pip install Pillow
!pip install numpy

import os
from getpass import getpass
from dotenv import load_dotenv

load_dotenv()

# Fallback ‚Äî ask user only if not already set
def ensure_env_key(key_name: str, prompt_text: str):
    """Prompt for API key only if not already set."""
    if not os.environ.get(key_name):
        os.environ[key_name] = getpass(f"Enter your {prompt_text}: ")

# Ensure each API key is present
ensure_env_key("OPENAI_API_KEY", "OpenAI API key")
ensure_env_key("GOOGLE_API_KEY", "Google API key")
ensure_env_key("HUGGINGFACEHUB_API_TOKEN", "Hugging Face token")
ensure_env_key("COHERE_API_KEY", "Cohere API key")

print("\nAll API keys loaded securely.")

from pathlib import Path

# Create data directories
Path("./data/tmp").mkdir(parents=True, exist_ok=True)
Path("./data/vector_stores").mkdir(parents=True, exist_ok=True)
Path("./data/docs").mkdir(parents=True, exist_ok=True)

print("Directories created successfully!")

from google.colab import files
import shutil

# Upload files to Colab
print("üìÅ Upload your documents (txt, pdf, csv, or docx):")
uploaded = files.upload()

# Move uploaded files to tmp directory
for filename in uploaded.keys():
    shutil.move(filename, f"./data/tmp/{filename}")
    print(f"Moved {filename} to ./data/tmp/")

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import os
from pathlib import Path
from IPython.display import Markdown

# Import all required libraries
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.llms import HuggingFaceHub
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma

# Set directories
TMP_DIR = Path("./data/tmp").resolve()
LOCAL_VECTOR_STORE_DIR = Path("./data/vector_stores").resolve()

print("Setup complete! Ready to build your RAG chatbot.")

!pip install pypdf

# Import document loaders
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader, DirectoryLoader, CSVLoader, Docx2txtLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

def load_documents(tmp_dir):
    documents = []

    # Load different file types
    for ext, loader in [
        ("**/*.txt", TextLoader),
        ("**/*.pdf", PyPDFLoader),
        ("**/*.csv", CSVLoader),
        ("**/*.docx", Docx2txtLoader)
    ]:
        loader_instance = DirectoryLoader(
            tmp_dir.as_posix(),
            glob=ext,
            loader_cls=loader,
            show_progress=True
        )
        documents.extend(loader_instance.load())

    return documents

# Load documents
documents = load_documents(TMP_DIR)
print(f"Loaded {len(documents)} documents")

# Split documents
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1600,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
print(f"Created {len(chunks)} chunks")

# Install if needed
!pip install sentence-transformers

from langchain_community.embeddings import HuggingFaceEmbeddings

# This runs locally - completely free!
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store - no rate limits!
vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./data/vector_stores/my_vectorstore"
)

print("Vector store created!")

# First, let's check what models are actually available with your API key
import google.generativeai as genai

genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

print("Available models for generateContent:")
for model in genai.list_models():
    if 'generateContent' in model.supported_generation_methods:
        print(f"  ‚úì {model.name}")

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# Initialize LLM (using Google Gemini)
# IMPORTANT: Set convert_system_message_to_human=True for Gemini
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash",  # ‚Üê Include "models/" prefix!
    google_api_key=os.environ['GOOGLE_API_KEY'],
    temperature=0.5,
    convert_system_message_to_human=True
)

# Create memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# Create retriever
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# Create conversational chain
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

print("Conversational RAG chain ready!")

def chat(question):
    # Use invoke() instead of __call__() to avoid deprecation warning
    response = chain.invoke({"question": question})
    print(f"Question: {question}")
    print(f"\nAnswer: {response['answer']}")
    print(f"\nSources: {len(response['source_documents'])} documents used")
    print("-" * 80)

    # Display each source chunk
    for i, doc in enumerate(response['source_documents'], 1):
        print(f"\n--- Source {i} ---")
        print(f"Content: {doc.page_content[:300]}...")  # First 300 characters
        print(f"Metadata: {doc.metadata}")  # Shows page number, file name, etc.

    return response

# Example usage
response = chat("What is this document about?")

# Ask follow-up questions
response = chat("Can you summarize the key points?")
response = chat("What are the main findings?")

